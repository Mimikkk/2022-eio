{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Elementy Inteligencji Obliczeniowej - Sieci Neuronowe\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Prowadzący:** Jakub Bednarek<br>\n",
    "**Kontakt:** jakub.bednarek@put.poznan.pl<br>\n",
    "**Materiały:** [Strona WWW](http://jakub.bednarek.pracownik.put.poznan.pl)\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0tVMrm99g5w"
   },
   "source": [
    "## Uwaga\n",
    "\n",
    "* **Aby wykonać polecenia należy najpierw przejść do trybu 'playground'. File -> Open in Playground Mode**\n",
    "* Nowe funkcje Colab pozwalają na autouzupełnianie oraz czytanie dokumentacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wlq47LA0BuBB"
   },
   "source": [
    "## Cel ćwiczeń:\n",
    "- zapoznanie się z rekurencyjnymi sieciami neuronowymi,\n",
    "- stworzenie modelu sieci z warstwami rekurencyjnymi dla zbioru danych MNIST,\n",
    "- stworzenie własnych implementacji warstwami neuronowych"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SxLU8paIDmUe",
    "outputId": "973390c4-5aee-4456-e4d9-f4c162717038",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "scL5_bHTD-M7"
   },
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D, LSTM, LSTMCell, \\\n",
    "  SimpleRNNCell\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adadelta, RMSprop\n",
    "from tensorflow.python.keras import backend as K\n",
    "import functools\n",
    "def pipe(*fns):\n",
    "  return functools.reduce(lambda f, g: lambda x: f(g(x)), reversed(fns))\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wV_u-YBWEJ8X",
    "outputId": "1125ba20-3008-4697-fe68-906e775b7359",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    }
   },
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')  # shape: 60000, 28, 28\n",
    "x_test = x_test.astype('float32')  # shape: 10000, 28, 28\n",
    "x_train /= 255  # normalizacja wartości do przedziału [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "y_train = to_categorical(y_train, 10)  # zamiana etykiety na one-hot encoding; np. 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y_test = to_categorical(y_test, 10)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppmDSGoyFuJ9"
   },
   "source": [
    "## Sieci rekurencyjne\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "https://www.tensorflow.org/guide/function\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "Przykładowy model z warstwą rekurencyjną dla danych MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ViqotGlHGy9t"
   },
   "source": [
    "class RecurrentModel(Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(RecurrentModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.lstm_1 = LSTM(128, activation='relu')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return pipe(\n",
    "      self.lstm_1,\n",
    "      self.dense_1,\n",
    "    )(inputs)\n",
    "\n",
    "model = RecurrentModel(num_classes=10)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jFs-QBFtEp0s",
    "outputId": "085c2e9a-e4ba-4de7-e818-18c3e02526e4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    }
   },
   "source": [
    "model.compile(optimizer=RMSprop(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=4)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 39s 20ms/step - loss: 0.5953 - accuracy: 0.8168\n",
      "Epoch 2/4\n",
      " 237/1875 [==>...........................] - ETA: 32s - loss: 0.1668 - accuracy: 0.9520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39mRMSprop(),\n\u001B[0;32m      2\u001B[0m               loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      3\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m----> 5\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32m~\\PycharmProjects\\eio-2022\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgtZzVYg1361"
   },
   "source": [
    "### Zadanie 1\n",
    "Rozszerz model z powyższego przykładu o kolejną warstwę rekurencyjną przed gęstą warstwą wyjściową.\n",
    "\n",
    "Standardowe sieci neuronowe generują jeden wynik na podstawie jednego inputu, natomiast sieci rekurencyjne przetwarzają dane sekwencyjnie, w każdym kroku łącząc wynik poprzedniego przetwarzania i aktualnego wejścia. Dlatego domyślnym wejściem sieci neuronowej jest tensor 3-wymiarowy ([batch_size,sequence_size,sample_size]).\n",
    "Domyślnie warstwy rekurencyjne w Kerasie zwracają tylko wyniki przetwarzania ostatniego\n",
    "kroku (otrzymują tensor 3-wymiarowy, zwracają tensor 2-wymiarowy). Jeśli chcesz zwrócić sekwencje wyników wszystkich kroków przetwarzania dla warstwy rekurencyjnej, musisz ustawić parametr return_sequences=True.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MSJUzxAc15uZ"
   },
   "source": [
    "class RecurrentBeans(Model):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(RecurrentBeans, self).__init__(name='RecurrentBeans')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.lstm_1 = LSTM(128, activation='relu', return_sequences=True)\n",
    "    self.lstm_2 = LSTM(128, activation='relu')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return pipe(\n",
    "      self.lstm_1,\n",
    "      self.lstm_2,\n",
    "      self.dense_1,\n",
    "    )(inputs)\n",
    "\n",
    "model = RecurrentBeans(num_classes=10)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=4)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.5242 - accuracy: 0.8385\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.1326 - accuracy: 0.9629\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.0920 - accuracy: 0.9750\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.0740 - accuracy: 0.9805\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x16657c57a30>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYDLWjdseB4H"
   },
   "source": [
    "### Zadanie 2 \n",
    "Wykorzystując model z przykładu, napisz sieć rekurencyjną przy użyciu SimpleRNNCell.\n",
    "\n",
    "Cell implementuje tylko operacje wykonywane przez warstwę\n",
    "rekurencyjną dla jednego kroku. Warstwy rekurencyjne w każdym kroku\n",
    "łączą wynik operacji poprzedniego kroku i aktualny input.\n",
    "Wykorzystaj pętle for do wielokrotnego wywołania komórki SimpleRNNCell (liczba kroków to liczba elementów w sekwencji). Aby wywołać SimpleRNNCell dla pojedynczego wejścia i stanu należy użyć jej metody ```call``` analogicznie jak w przypadku własnych modeli (tzn. ```my_model(input)```). \n",
    "\n",
    "\n",
    "\n",
    "Wywołanie zainicjalizowanej komórki rekurencyjnej wymaga podania aktualnego inputu i **listy macierzy** (w dokumentacji jest błąd, że ma to być macierz) stanów ukrytych poprzedniego kroku (SimpleRNNCell ma jeden stan, LSTMCell w następnym zadaniu ma dwa stany).\n",
    "\n",
    "Trzeba zainicjalizować ukryty stan warstwy wartościami początkowymi (można wykorzystać rozkład normalny - tf.random.normal)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6yZ8yKmbee44"
   },
   "source": [
    "class RNNRecurrentBeans(Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__(name='RNNRecurrentBeans')\n",
    "    self.num_classes = num_classes\n",
    "    self.cell = SimpleRNNCell(128, activation='relu')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    h = tf.random.normal([inputs.shape[0], self.cell.units])\n",
    "\n",
    "    for n in range(inputs.shape[1]):\n",
    "      x, h = self.cell(inputs[:, n, :], h)\n",
    "\n",
    "    return self.dense_1(x)\n",
    "\n",
    "model = RNNRecurrentBeans(num_classes=10)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=4)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4923 - accuracy: 0.8440\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2157 - accuracy: 0.9363\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1720 - accuracy: 0.9518\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1478 - accuracy: 0.9589\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1665afb3040>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyPGkC6oiEd5"
   },
   "source": [
    "### Zadanie 3\n",
    "Zamień komórkę rekurencyjną z poprzedniego zadania na LSTMCell (LSTMCell ma dwa stany ukryte)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C5MPQ1UcigN5"
   },
   "source": [
    "class LSTMRecurrentBeans(Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__(name='LSTMRecurrentBeans')\n",
    "    self.num_classes = num_classes\n",
    "    self.cell = LSTMCell(128, activation='relu')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    shape = (inputs.shape[0], self.cell.units)\n",
    "    h = [tf.random.normal(shape), tf.random.normal(shape)]\n",
    "\n",
    "    for n in range(inputs.shape[1]):\n",
    "      x, h = self.cell(inputs[:, n, :], h)\n",
    "\n",
    "    return self.dense_1(x)\n",
    "\n",
    "model = LSTMRecurrentBeans(num_classes=10)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=4)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.5712 - accuracy: 0.8183\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1448 - accuracy: 0.9588\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1008 - accuracy: 0.9727\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0900 - accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1665d1bdab0>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prwjaEv2efs3"
   },
   "source": [
    "### Zadanie 4\n",
    "Wykorzystując model z poprzedniego zadania, stwórz model sieci\n",
    "neuronowej z własną implementacją prostej warstwy rekurencyjnej.\n",
    "- w call zamień self.lstm_cell_layer(x) na wywołanie własnej metody np. self.cell(x)\n",
    "- w konstruktorze modelu usuń inicjalizację komórki LSTM i zastąp ją inicjalizacją warstw potrzebnych do stworzenia własnej komórki rekurencyjnej,\n",
    "- stwórz metodę cell() wykonującą operacje warstwy rekurencyjnej,\n",
    "- prosta warstwa rekurencyjna konkatenuje poprzedni wyniki i aktualny input, a następnie przepuszcza ten połączony tensor przez warstwę gęstą (Dense)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BGQr50EafxSH"
   },
   "source": [
    "class CoolRecurrentBeans(Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__(name='CoolRecurrentBeans')\n",
    "    self.num_classes = num_classes\n",
    "    self._cell = Dense(128, activation='relu')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def cell(self, x, h):\n",
    "    return self._cell(K.concatenate([x, h]))\n",
    "\n",
    "  def call(self, inputs):\n",
    "    h = tf.random.normal((inputs.shape[0], self._cell.units))\n",
    "\n",
    "    for n in range(inputs.shape[1]):\n",
    "      h = self.cell(inputs[:, n, :], h)\n",
    "\n",
    "    return self.dense_1(h)\n",
    "\n",
    "model = CoolRecurrentBeans(num_classes=10)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=4)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5619 - accuracy: 0.8217\n",
      "Epoch 2/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2644 - accuracy: 0.9223\n",
      "Epoch 3/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1971 - accuracy: 0.9436\n",
      "Epoch 4/4\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1662 - accuracy: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1665f5d0850>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3sOaUu3b77l"
   },
   "source": [
    "### Zadanie 5\n",
    "\n",
    "Na podstawie modelu z poprzedniego zadania stwórz model z własną implementacją warstwy LSTM. Dokładny i zrozumiały opis działania warstwy LSTM znajduje się na [stronie](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Kyu4YijDcA13"
   },
   "source": [
    "class LSTMCoolRecurrentBeans(Model):\n",
    "  def __init__(self, num_classes):\n",
    "    super().__init__(name='Model5')\n",
    "    self.num_classes = num_classes\n",
    "    self.units = 128\n",
    "    self.dense_i = Dense(self.units, activation='sigmoid')\n",
    "    self.dense_f = Dense(self.units, activation='sigmoid')\n",
    "    self.dense_o = Dense(self.units, activation='sigmoid')\n",
    "    self.dense_c = Dense(self.units, activation='tanh')\n",
    "    self.dense_1 = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def cell(self, x, h):\n",
    "    c_last, h_last = h\n",
    "    hx = K.concatenate([h_last, x])\n",
    "    c_next = self.dense_f(hx) * c_last + self.dense_i(hx) * self.dense_c(hx)\n",
    "    h_next = self.dense_o(hx) * K.tanh(c_next)\n",
    "    return h_next, (c_next, h_next)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    shape = (inputs.shape[0], self.units)\n",
    "    h = [tf.random.normal(shape), tf.random.normal(shape)]\n",
    "\n",
    "    for n in range(inputs.shape[1]):\n",
    "      x, h = self.cell(inputs[:, n, :], h)\n",
    "\n",
    "    return self.dense_1(x)\n",
    "\n",
    "model = LSTMCoolRecurrentBeans(num_classes=10)\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLSTMCoolRecurrentBeans\u001B[39;00m(\u001B[43mModel\u001B[49m):\n\u001B[0;32m      2\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_classes):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mModel5\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Model' is not defined"
     ]
    }
   ]
  }
 ]
}
